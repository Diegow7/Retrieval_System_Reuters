{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Bimestral: Sistema de Recuperaci´on de Información basado en Reuters-21578\n",
    "## 1. Introducción\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578.\n",
    "## 2. Fases del Proyecto\n",
    "###  2.1. Adquisición de datos\n",
    "\n",
    "*   Descargar el Corpus Reuters-21578\n",
    "*   Descomprimir y organizar los archivos\n",
    "*   Documentar el proceso de adquisición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descripción de la imagen](images/corpus_reuters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarga y análisis inicial del corpus\n",
    "\n",
    "1. El archivo comprimido fue descargado y descomprimido en una carpeta local vinculada a un repositorio de GitHub.\n",
    "2. Posteriormente, se analizó el contenido del corpus, obteniendo los siguientes resultados:\n",
    "   - **Carpeta `test`**: Contiene 3019 archivos.\n",
    "   - **Carpeta `training`**: Contiene 7769 archivos.\n",
    "   - **Archivo `cats`**: Incluye las categorías.\n",
    "   - **Archivo `readme`**: Proporciona información general sobre el corpus.\n",
    "   - **Archivo `stopwords`**: Contiene una lista de palabras vacías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install rarfile` para tener la biblioteca `rarfile` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install nltk` para tener la biblioteca `nltk` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import rarfile\n",
    "import subprocess\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Extraer el contenido relevante de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos extraídos:\n",
      "['reuters']\n"
     ]
    }
   ],
   "source": [
    "# Configurar la herramienta unrar\n",
    "rarfile.UNRAR_TOOL = r\"D:\\UnRAR\\UnRAR.exe\"\n",
    "\n",
    "# Ruta del archivo .rar\n",
    "rar_path = 'material/reuters.rar'\n",
    "output_dir = 'material/content'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Descomprimir el archivo .rar\n",
    "with rarfile.RarFile(rar_path) as rf:\n",
    "    rf.extractall(output_dir)\n",
    "\n",
    "# Verificar que se haya descomprimido correctamente\n",
    "print(\"Archivos extraídos:\")\n",
    "print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de documentos\n",
    "train_dir = 'material/content/reuters/training'\n",
    "test_dir = 'material/content/reuters/test'\n",
    "cats_file = 'material/content/reuters/cats.txt'\n",
    "\n",
    "# Diccionario para almacenar documentos\n",
    "documentos = {}\n",
    "\n",
    "# Función para extraer contenido de un archivo de noticias\n",
    "def extraer_texto(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            contenido = file.read()\n",
    "            texto_limpio = contenido.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "            return texto_limpio\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Función para cargar las categorías de los documentos\n",
    "def cargar_categorias(filepath):\n",
    "    categorias = {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            for linea in file:\n",
    "                partes = linea.strip().split()  # Divide la línea en partes separadas por espacios\n",
    "                if len(partes) >= 2:\n",
    "                    doc_id = partes[0]  # Primer elemento es el id del documento (test/14826, training/1)\n",
    "                    etiquetas = partes[1:]  # Resto de elementos son categorías\n",
    "                    categorias[doc_id] = etiquetas\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo de categorías {filepath}: {e}\")\n",
    "    return categorias\n",
    "\n",
    "# Función para cargar los documentos y asociar categorías\n",
    "def cargar_documentos(directorio, tipo, categorias_dict):\n",
    "    archivos = os.listdir(directorio)\n",
    "    if not archivos:\n",
    "        print(f\"No se encontraron archivos en {directorio}\")\n",
    "    for archivo in archivos:\n",
    "        filepath = os.path.join(directorio, archivo)\n",
    "        doc_id = f\"{tipo}/{archivo}\"\n",
    "        texto = extraer_texto(filepath)\n",
    "        categorias = categorias_dict.get(doc_id, [])  # Obtener categorías, si no hay, devuelve lista vacía\n",
    "\n",
    "        # Almacenar en el diccionario\n",
    "        documentos[doc_id] = {\n",
    "            \"texto\": texto,\n",
    "            \"categorias\": categorias\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 10788\n",
      "ID: training/1\n",
      "Texto: BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating the drought since early\n",
      "  January and improving prospects for the coming temporao,\n",
      "  although normal humidity levels have not been restored,\n",
      "  Comissaria Smith said in its weekly review.\n",
      "      The dry period means the temporao will be late this year.\n",
      "      Arrivals for the week ended February 22 were 155,221 bags\n",
      "  of 60 kilos making a cumulative total for the season of 5.93\n",
      "  mln against 5.81 at th...\n",
      "Categorías: ['cocoa']\n"
     ]
    }
   ],
   "source": [
    "# Cargar categorías\n",
    "categorias_dict = cargar_categorias(cats_file)\n",
    "\n",
    "# Cargar documentos de entrenamiento y prueba\n",
    "cargar_documentos(train_dir, 'training', categorias_dict)\n",
    "cargar_documentos(test_dir, 'test', categorias_dict)\n",
    "\n",
    "# Verificar el tamaño y algunas muestras\n",
    "print(f\"Total de documentos cargados: {len(documentos)}\")\n",
    "if documentos:\n",
    "    ejemplo_doc = list(documentos.items())[0]\n",
    "    print(f\"ID: {ejemplo_doc[0]}\")\n",
    "    print(f\"Texto: {ejemplo_doc[1]['texto'][:500]}...\")  # Mostrar los primeros 500 caracteres\n",
    "    print(f\"Categorías: {ejemplo_doc[1]['categorias']}\")\n",
    "else:\n",
    "    print(\"No se cargaron documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Realizar limpieza de datos: eliminación de caracteres no deseados, normalización de texto, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Tokenización: dividir el texto en palabras o tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Eliminar stop words y  aplicar stemming o lematización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Representación de Datos en Espacio Vectorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
