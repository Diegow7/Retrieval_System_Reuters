{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Bimestral: Sistema de Recuperaci´on de Información basado en Reuters-21578\n",
    "## 1. Introducción\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578.\n",
    "## 2. Fases del Proyecto\n",
    "###  2.1. Adquisición de datos\n",
    "\n",
    "*   Descargar el Corpus Reuters-21578\n",
    "*   Descomprimir y organizar los archivos\n",
    "*   Documentar el proceso de adquisición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descripción de la imagen](images/corpus_reuters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarga y análisis inicial del corpus\n",
    "\n",
    "1. El archivo comprimido fue descargado y descomprimido en una carpeta local vinculada a un repositorio de GitHub.\n",
    "2. Posteriormente, se analizó el contenido del corpus, obteniendo los siguientes resultados:\n",
    "   - **Carpeta `test`**: Contiene 3019 archivos.\n",
    "   - **Carpeta `training`**: Contiene 7769 archivos.\n",
    "   - **Archivo `cats`**: Incluye las categorías.\n",
    "   - **Archivo `readme`**: Proporciona información general sobre el corpus.\n",
    "   - **Archivo `stopwords`**: Contiene una lista de palabras vacías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install rarfile` para tener la biblioteca `rarfile` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install nltk` para tener la biblioteca `nltk` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import rarfile\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Extraer el contenido relevante de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos extraídos:\n",
      "['reuters']\n"
     ]
    }
   ],
   "source": [
    "rarfile.UNRAR_TOOL = r\"D:\\UnRAR\\UnRAR.exe\" # Cambiar por ubicación local de tu herramienta UNRAR\n",
    "\n",
    "# Ruta del archivo .rar\n",
    "rar_path = 'material/reuters.rar'\n",
    "output_dir = 'material/content'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Descomprimir el archivo .rar\n",
    "with rarfile.RarFile(rar_path) as rf:\n",
    "    rf.extractall(output_dir)\n",
    "\n",
    "# Verificar que se haya descomprimido correctamente\n",
    "print(\"Archivos extraídos:\")\n",
    "print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de documentos\n",
    "train_dir = 'material/content/reuters/training'\n",
    "test_dir = 'material/content/reuters/test'\n",
    "cats_file = 'material/content/reuters/cats.txt'\n",
    "\n",
    "# Diccionario para almacenar documentos\n",
    "documentos = {}\n",
    "\n",
    "# Función para extraer contenido de un archivo de noticias\n",
    "def extraer_texto(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            contenido = file.read()\n",
    "            texto_limpio = contenido.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "            return texto_limpio\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Función para cargar las categorías de los documentos\n",
    "def cargar_categorias(filepath):\n",
    "    categorias = {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            for linea in file:\n",
    "                partes = linea.strip().split()  # Divide la línea en partes separadas por espacios\n",
    "                if len(partes) >= 2:\n",
    "                    doc_id = partes[0]  # Primer elemento es el id del documento (test/14826, training/1)\n",
    "                    etiquetas = partes[1:]  # Resto de elementos son categorías\n",
    "                    categorias[doc_id] = etiquetas\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo de categorías {filepath}: {e}\")\n",
    "    return categorias\n",
    "\n",
    "# Función para cargar los documentos y asociar categorías\n",
    "def cargar_documentos(directorio, tipo, categorias_dict):\n",
    "    archivos = os.listdir(directorio)\n",
    "    if not archivos:\n",
    "        print(f\"No se encontraron archivos en {directorio}\")\n",
    "    for archivo in archivos:\n",
    "        filepath = os.path.join(directorio, archivo)\n",
    "        doc_id = f\"{tipo}/{archivo}\"\n",
    "        texto = extraer_texto(filepath)\n",
    "        categorias = categorias_dict.get(doc_id, [])  # Obtener categorías, si no hay, devuelve lista vacía\n",
    "\n",
    "        # Almacenar en el diccionario\n",
    "        documentos[doc_id] = {\n",
    "            \"texto\": texto,\n",
    "            \"categorias\": categorias\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 10788\n",
      "ID: training/1\n",
      "Texto: BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating the drought since early\n",
      "  January and improving prospects for the coming temporao,\n",
      "  although normal humidity levels have not been restored,\n",
      "  Comissaria Smith said in its weekly review.\n",
      "      The dry period means the temporao will be late this year.\n",
      "      Arrivals for the week ended February 22 were 155,221 bags\n",
      "  of 60 kilos making a cumulative total for the season of 5.93\n",
      "  mln against 5.81 at th...\n",
      "Categorías: ['cocoa']\n"
     ]
    }
   ],
   "source": [
    "# Cargar categorías\n",
    "categorias_dict = cargar_categorias(cats_file)\n",
    "\n",
    "# Cargar documentos de entrenamiento y prueba\n",
    "cargar_documentos(train_dir, 'training', categorias_dict)\n",
    "cargar_documentos(test_dir, 'test', categorias_dict)\n",
    "\n",
    "# Verificar el tamaño y algunas muestras\n",
    "print(f\"Total de documentos cargados: {len(documentos)}\")\n",
    "if documentos:\n",
    "    ejemplo_doc = list(documentos.items())[0]\n",
    "    print(f\"ID: {ejemplo_doc[0]}\")\n",
    "    print(f\"Texto: {ejemplo_doc[1]['texto'][:500]}...\")  # Mostrar los primeros 500 caracteres\n",
    "    print(f\"Categorías: {ejemplo_doc[1]['categorias']}\")\n",
    "else:\n",
    "    print(\"No se cargaron documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Realizar limpieza de datos: eliminación de caracteres no deseados, normalización de texto, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    # 1. Conversión a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # 2. Eliminación de caracteres especiales y números\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "\n",
    "    # 3. Eliminación de espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Texto limpio: bahia cocoa review showers continued throughout the week in the bahia cocoa zone alleviating the drought since early january and improving prospects for the coming temporao although normal humidity levels have not been restored comissaria smith said in its weekly review the dry period means the temporao will be late this year arrivals for the week ended february were bags of kilos making a cumulative total for the season of mln against at the same stage last year again it seems that cocoa delive...\n"
     ]
    }
   ],
   "source": [
    "# Aplicar limpieza de texto a todos los documentos\n",
    "for doc_id in documentos:\n",
    "    texto_original = documentos[doc_id]['texto']\n",
    "    texto_limpio = limpiar_texto(texto_original)\n",
    "\n",
    "    # Actualizar el texto limpio en el diccionario\n",
    "    documentos[doc_id]['texto'] = texto_limpio\n",
    "\n",
    "# Verificar el resultado de la limpieza en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Texto limpio: {ejemplo_doc[1]['texto'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Tokenización: dividir el texto en palabras o tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para tokenizar texto\n",
    "def tokenizar_texto(texto):\n",
    "    # Utilizamos word_tokenize de NLTK para dividir en tokens\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens: ['bahia', 'cocoa', 'review', 'showers', 'continued', 'throughout', 'the', 'week', 'in', 'the', 'bahia', 'cocoa', 'zone', 'alleviating', 'the', 'drought', 'since', 'early', 'january', 'and']...\n"
     ]
    }
   ],
   "source": [
    "# Aplicar tokenización a todos los documentos\n",
    "for doc_id in documentos:\n",
    "    texto_limpio = documentos[doc_id]['texto']\n",
    "    tokens = tokenizar_texto(texto_limpio)\n",
    "\n",
    "    # Guardamos los tokens en el diccionario\n",
    "    documentos[doc_id]['tokens'] = tokens\n",
    "\n",
    "# Verificar el resultado de la tokenización en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Eliminar stop words y  aplicar stemming o lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de Stop Words cargadas: ['rather', 'however', 'willing', 'q', 'secondly', 'but', 'specify', 'you', 'own', 'know']\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo de stopwords proporcionado\n",
    "ruta_stopwords = 'material/content/reuters/stopwords'\n",
    "stopwords_personalizadas = set()\n",
    "\n",
    "with open(ruta_stopwords, 'r') as archivo:\n",
    "    for linea in archivo:\n",
    "        palabra = linea.strip()  # Removemos espacios y saltos de línea\n",
    "        if palabra:  # Evitamos añadir líneas vacías\n",
    "            stopwords_personalizadas.add(palabra.lower())\n",
    "\n",
    "# Verificamos algunas stop words cargadas\n",
    "print(\"Ejemplo de Stop Words cargadas:\", list(stopwords_personalizadas)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens sin Stop Words: ['bahia', 'cocoa', 'review', 'showers', 'continued', 'week', 'bahia', 'cocoa', 'zone', 'alleviating', 'drought', 'early', 'january', 'improving', 'prospects', 'coming', 'temporao', 'normal', 'humidity', 'levels']...\n"
     ]
    }
   ],
   "source": [
    "# Función para eliminar stop words de los tokens\n",
    "def eliminar_stopwords(tokens, stopwords):\n",
    "    # Filtramos los tokens que no están en la lista de stopwords\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords]\n",
    "    return tokens_filtrados\n",
    "\n",
    "# Aplicamos la eliminación de stop words a cada documento\n",
    "for doc_id in documentos:\n",
    "    tokens = documentos[doc_id]['tokens']\n",
    "    tokens_sin_stopwords = eliminar_stopwords(tokens, stopwords_personalizadas)\n",
    "\n",
    "    # Guardamos los tokens filtrados\n",
    "    documentos[doc_id]['tokens'] = tokens_sin_stopwords\n",
    "\n",
    "# Verificar el resultado después de eliminar stop words en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens sin Stop Words: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Para nuestro caso de estudio y aplicación de este proyecto, aplicaremos **Steming**. \n",
    "\n",
    "Como equipo, consideramos más eficiente en términos de recursos porque el stemming utiliza reglas más simples y rápidas para reducir las palabras a su raíz, mientras que la lemmatización requiere un procesamiento más complejo y el uso de diccionarios para obtener la forma base correcta de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Función para aplicar stemming a los tokens de un documento\n",
    "def aplicar_stemming(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens con Stemming: ['bahia', 'cocoa', 'review', 'shower', 'continu', 'week', 'bahia', 'cocoa', 'zone', 'allevi', 'drought', 'earli', 'januari', 'improv', 'prospect', 'come', 'temporao', 'normal', 'humid', 'level']...\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos el stemming a los tokens de cada documento\n",
    "for doc_id in documentos:\n",
    "    tokens = documentos[doc_id]['tokens']  # Obtiene los tokens del documento\n",
    "    tokens_stemmed = aplicar_stemming(tokens)  # Aplica el stemming\n",
    "    documentos[doc_id]['tokens'] = tokens_stemmed  # Guarda los tokens procesados\n",
    "\n",
    "# Verificamos un documento de ejemplo después del stemming\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens con Stemming: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Representación de Datos en Espacio Vectorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
