{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Bimestral: Sistema de Recuperaci´on de Información basado en Reuters-21578\n",
    "## 1. Introducción\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578.\n",
    "## 2. Fases del Proyecto\n",
    "###  2.1. Adquisición de datos\n",
    "\n",
    "*   Descargar el Corpus Reuters-21578\n",
    "*   Descomprimir y organizar los archivos\n",
    "*   Documentar el proceso de adquisición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descripción de la imagen](images/corpus_reuters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarga y análisis inicial del corpus\n",
    "\n",
    "1. El archivo comprimido fue descargado y descomprimido en una carpeta local vinculada a un repositorio de GitHub.\n",
    "2. Posteriormente, se analizó el contenido del corpus, obteniendo los siguientes resultados:\n",
    "   - **Carpeta `test`**: Contiene 3019 archivos.\n",
    "   - **Carpeta `training`**: Contiene 7769 archivos.\n",
    "   - **Archivo `cats`**: Incluye las categorías.\n",
    "   - **Archivo `readme`**: Proporciona información general sobre el corpus.\n",
    "   - **Archivo `stopwords`**: Contiene una lista de palabras vacías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerias y Dependencias necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install rarfile` para tener la biblioteca `rarfile` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install nltk` para tener la biblioteca `nltk` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comando `pip install scikit-learn` instala la librería `scikit-learn`, que es una herramienta para realizar tareas de aprendizaje automático (machine learning) como clasificación, regresión y clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comando `pip install gensim` instala la librería `gensim`, que se utiliza para el procesamiento de texto y la creación de modelos de aprendizaje automático, como Word2Vec, para generar representaciones vectoriales de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import rarfile\n",
    "import time # Evaluar tecnicas de vectorización\n",
    "import psutil # Evaluar tecnicas de vectorización\n",
    "import gc # Evaluar tecnicas de vectorización\n",
    "import numpy as np\n",
    "from collections import defaultdict # Para el indice invertido\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Extraer el contenido relevante de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos extraídos:\n",
      "['reuters']\n"
     ]
    }
   ],
   "source": [
    "rarfile.UNRAR_TOOL = r\"D:\\UnRAR\\UnRAR.exe\" # Cambiar por ubicación local de tu herramienta UNRAR\n",
    "\n",
    "# Ruta del archivo .rar\n",
    "rar_path = 'material/reuters.rar'\n",
    "output_dir = 'material/content'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Descomprimir el archivo .rar\n",
    "with rarfile.RarFile(rar_path) as rf:\n",
    "    rf.extractall(output_dir)\n",
    "\n",
    "# Verificar que se haya descomprimido correctamente\n",
    "print(\"Archivos extraídos:\")\n",
    "print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de documentos\n",
    "train_dir = 'material/content/reuters/training'\n",
    "test_dir = 'material/content/reuters/test'\n",
    "cats_file = 'material/content/reuters/cats.txt'\n",
    "\n",
    "# Diccionario para almacenar documentos\n",
    "documentos = {}\n",
    "\n",
    "# Función para extraer contenido de un archivo de noticias\n",
    "def extraer_texto(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            contenido = file.read()\n",
    "            texto_limpio = contenido.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "            return texto_limpio\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Función para cargar las categorías de los documentos\n",
    "def cargar_categorias(filepath):\n",
    "    categorias = {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            for linea in file:\n",
    "                partes = linea.strip().split()  # Divide la línea en partes separadas por espacios\n",
    "                if len(partes) >= 2:\n",
    "                    doc_id = partes[0]  # Primer elemento es el id del documento (test/14826, training/1)\n",
    "                    etiquetas = partes[1:]  # Resto de elementos son categorías\n",
    "                    categorias[doc_id] = etiquetas\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo de categorías {filepath}: {e}\")\n",
    "    return categorias\n",
    "\n",
    "# Función para cargar los documentos y asociar categorías\n",
    "def cargar_documentos(directorio, tipo, categorias_dict):\n",
    "    archivos = os.listdir(directorio)\n",
    "    if not archivos:\n",
    "        print(f\"No se encontraron archivos en {directorio}\")\n",
    "    for archivo in archivos:\n",
    "        filepath = os.path.join(directorio, archivo)\n",
    "        doc_id = f\"{tipo}/{archivo}\"\n",
    "        texto = extraer_texto(filepath)\n",
    "        categorias = categorias_dict.get(doc_id, [])  # Obtener categorías, si no hay, devuelve lista vacía\n",
    "\n",
    "        # Almacenar en el diccionario\n",
    "        documentos[doc_id] = {\n",
    "            \"texto\": texto,\n",
    "            \"categorias\": categorias\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 10788\n",
      "ID: training/1\n",
      "Texto: BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating the drought since early\n",
      "  January and improving prospects for the coming temporao,\n",
      "  although normal humidity levels have not been restored,\n",
      "  Comissaria Smith said in its weekly review.\n",
      "      The dry period means the temporao will be late this year.\n",
      "      Arrivals for the week ended February 22 were 155,221 bags\n",
      "  of 60 kilos making a cumulative total for the season of 5.93\n",
      "  mln against 5.81 at th...\n",
      "Categorías: ['cocoa']\n"
     ]
    }
   ],
   "source": [
    "# Cargar categorías\n",
    "categorias_dict = cargar_categorias(cats_file)\n",
    "\n",
    "# Cargar documentos de entrenamiento y prueba\n",
    "cargar_documentos(train_dir, 'training', categorias_dict)\n",
    "cargar_documentos(test_dir, 'test', categorias_dict)\n",
    "\n",
    "# Verificar el tamaño y algunas muestras\n",
    "print(f\"Total de documentos cargados: {len(documentos)}\")\n",
    "if documentos:\n",
    "    ejemplo_doc = list(documentos.items())[0]\n",
    "    print(f\"ID: {ejemplo_doc[0]}\")\n",
    "    print(f\"Texto: {ejemplo_doc[1]['texto'][:500]}...\")  # Mostrar los primeros 500 caracteres\n",
    "    print(f\"Categorías: {ejemplo_doc[1]['categorias']}\")\n",
    "else:\n",
    "    print(\"No se cargaron documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Realizar limpieza de datos: eliminación de caracteres no deseados, normalización de texto, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    # 1. Conversión a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # 2. Eliminación de caracteres especiales y números\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "\n",
    "    # 3. Eliminación de espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Texto limpio: bahia cocoa review showers continued throughout the week in the bahia cocoa zone alleviating the drought since early january and improving prospects for the coming temporao although normal humidity levels have not been restored comissaria smith said in its weekly review the dry period means the temporao will be late this year arrivals for the week ended february were bags of kilos making a cumulative total for the season of mln against at the same stage last year again it seems that cocoa delive...\n"
     ]
    }
   ],
   "source": [
    "# Aplicar limpieza de texto a todos los documentos\n",
    "for doc_id in documentos:\n",
    "    texto_original = documentos[doc_id]['texto']\n",
    "    texto_limpio = limpiar_texto(texto_original)\n",
    "\n",
    "    # Actualizar el texto limpio en el diccionario\n",
    "    documentos[doc_id]['texto'] = texto_limpio\n",
    "\n",
    "# Verificar el resultado de la limpieza en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Texto limpio: {ejemplo_doc[1]['texto'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Tokenización: dividir el texto en palabras o tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `nltk.download('punkt')` o `nltk.download('punkt_tab')`, por lo cual no es necesario volver a ejcutarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar los recursos necesarios de NLTK para tokenizar texto en oraciones y palabras\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para tokenizar texto\n",
    "def tokenizar_texto(texto):\n",
    "    # Utilizamos word_tokenize de NLTK para dividir en tokens\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens: ['bahia', 'cocoa', 'review', 'showers', 'continued', 'throughout', 'the', 'week', 'in', 'the', 'bahia', 'cocoa', 'zone', 'alleviating', 'the', 'drought', 'since', 'early', 'january', 'and']...\n"
     ]
    }
   ],
   "source": [
    "# Aplicar tokenización a todos los documentos\n",
    "for doc_id in documentos:\n",
    "    texto_limpio = documentos[doc_id]['texto']\n",
    "    tokens = tokenizar_texto(texto_limpio)\n",
    "\n",
    "    # Guardamos los tokens en el diccionario\n",
    "    documentos[doc_id]['tokens'] = tokens\n",
    "\n",
    "# Verificar el resultado de la tokenización en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Eliminar stop words y  aplicar stemming o lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de Stop Words cargadas: ['just', 'goes', 'overall', 'why', 'went', 'somewhat', 'plus', \"what's\", 'are', 'everywhere']\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo de stopwords proporcionado\n",
    "ruta_stopwords = 'material/content/reuters/stopwords'\n",
    "stopwords_personalizadas = set()\n",
    "\n",
    "with open(ruta_stopwords, 'r') as archivo:\n",
    "    for linea in archivo:\n",
    "        palabra = linea.strip()  # Removemos espacios y saltos de línea\n",
    "        if palabra:  # Evitamos añadir líneas vacías\n",
    "            stopwords_personalizadas.add(palabra.lower())\n",
    "\n",
    "# Verificamos algunas stop words cargadas\n",
    "print(\"Ejemplo de Stop Words cargadas:\", list(stopwords_personalizadas)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens sin Stop Words: ['bahia', 'cocoa', 'review', 'showers', 'continued', 'week', 'bahia', 'cocoa', 'zone', 'alleviating', 'drought', 'early', 'january', 'improving', 'prospects', 'coming', 'temporao', 'normal', 'humidity', 'levels']...\n"
     ]
    }
   ],
   "source": [
    "# Función para eliminar stop words de los tokens\n",
    "def eliminar_stopwords(tokens, stopwords):\n",
    "    # Filtramos los tokens que no están en la lista de stopwords\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords]\n",
    "    return tokens_filtrados\n",
    "\n",
    "# Aplicamos la eliminación de stop words a cada documento\n",
    "for doc_id in documentos:\n",
    "    tokens = documentos[doc_id]['tokens']\n",
    "    tokens_sin_stopwords = eliminar_stopwords(tokens, stopwords_personalizadas)\n",
    "\n",
    "    # Guardamos los tokens filtrados\n",
    "    documentos[doc_id]['tokens'] = tokens_sin_stopwords\n",
    "\n",
    "# Verificar el resultado después de eliminar stop words en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens sin Stop Words: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Para nuestro caso de estudio y aplicación de este proyecto, aplicaremos **Steming**. \n",
    "\n",
    "Como equipo, consideramos más eficiente en términos de recursos porque el stemming utiliza reglas más simples y rápidas para reducir las palabras a su raíz, mientras que la lemmatización requiere un procesamiento más complejo y el uso de diccionarios para obtener la forma base correcta de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Función para aplicar stemming a los tokens de un documento\n",
    "def aplicar_stemming(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens con Stemming: ['bahia', 'cocoa', 'review', 'shower', 'continu', 'week', 'bahia', 'cocoa', 'zone', 'allevi', 'drought', 'earli', 'januari', 'improv', 'prospect', 'come', 'temporao', 'normal', 'humid', 'level']...\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos el stemming a los tokens de cada documento\n",
    "for doc_id in documentos:\n",
    "    tokens = documentos[doc_id]['tokens']  # Obtiene los tokens del documento\n",
    "    tokens_stemmed = aplicar_stemming(tokens)  # Aplica el stemming\n",
    "    documentos[doc_id]['tokens'] = tokens_stemmed  # Guarda los tokens procesados\n",
    "\n",
    "# Verificamos un documento de ejemplo después del stemming\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens con Stemming: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Representación de Datos en Espacio Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Utilizar técnicas como Bag of Words (BoW), TF-IDF, y Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características (Palabras) BoW: ['aa' 'aaa' 'aabex' 'aachen' 'aaminu' 'aancor' 'aap' 'aaplu' 'aar'\n",
      " 'aarnoud']...\n",
      "Forma de la matriz BoW: (10788, 26004)\n"
     ]
    }
   ],
   "source": [
    "# Crear un objeto de CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Obtener las palabras de los documentos (usando los textos limpios después de eliminar stopwords y stemming)\n",
    "documentos_texto = [\" \".join(doc[\"tokens\"]) for doc in documentos.values()]\n",
    "\n",
    "# Ajustar el modelo y transformar los documentos\n",
    "X_bow = vectorizer.fit_transform(documentos_texto)\n",
    "\n",
    "# Mostrar las características (palabras)\n",
    "print(f\"Características (Palabras) BoW: {vectorizer.get_feature_names_out()[:10]}...\")  # Mostrar las primeras 10 palabras\n",
    "\n",
    "# Mostrar la matriz de términos de frecuencia\n",
    "print(f\"Forma de la matriz BoW: {X_bow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características (Palabras) TF-IDF: ['aa' 'aaa' 'aabex' 'aachen' 'aaminu' 'aancor' 'aap' 'aaplu' 'aar'\n",
      " 'aarnoud']...\n",
      "Forma de la matriz TF-IDF: (10788, 26004)\n"
     ]
    }
   ],
   "source": [
    "# Crear el objeto TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar y transformar los documentos\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documentos_texto)\n",
    "\n",
    "# Mostrar las características (palabras)\n",
    "print(f\"Características (Palabras) TF-IDF: {tfidf_vectorizer.get_feature_names_out()[:10]}...\")  # Primeras 10 palabras\n",
    "\n",
    "# Mostrar la matriz de TF-IDF\n",
    "print(f\"Forma de la matriz TF-IDF: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del vector Word2Vec: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo Word2Vec\n",
    "model_w2v = Word2Vec(sentences=[doc[\"tokens\"] for doc in documentos.values()], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Función para obtener el vector medio de un documento\n",
    "def obtener_vector_promedio(tokens, model):\n",
    "    # Extraer los vectores de las palabras, promediarlos y retornar el vector\n",
    "    vectores = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectores:\n",
    "        return np.mean(vectores, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Devuelve un vector cero si no hay palabras del vocabulario en el modelo\n",
    "\n",
    "# Obtener los vectores para cada documento\n",
    "documentos_w2v = {}\n",
    "for doc_id, doc in documentos.items():\n",
    "    vector_promedio = obtener_vector_promedio(doc['tokens'], model_w2v)\n",
    "    documentos_w2v[doc_id] = vector_promedio\n",
    "\n",
    "# Verificar el tamaño de los vectores\n",
    "print(f\"Dimensiones del vector Word2Vec: {documentos_w2v[next(iter(documentos_w2v))].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Evaluar las diferentes técnicas de vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación BoW: Tiempo = 0.9270 segundos, Memoria = 7.77 MB\n",
      "Evaluación TF-IDF: Tiempo = 0.9977 segundos, Memoria = 3.73 MB\n",
      "Evaluación Word2Vec: Tiempo = 6.8307 segundos, Memoria = 4.65 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para medir el tiempo y la memoria\n",
    "def medir_tiempo_memoria(func):\n",
    "    # Medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Medir la memoria antes de ejecutar\n",
    "    proceso = psutil.Process()\n",
    "    memoria_inicial = proceso.memory_info().rss  # Memoria en bytes\n",
    "    \n",
    "    # Ejecutar la función\n",
    "    result = func()\n",
    "    \n",
    "    # Medir el tiempo y la memoria después de ejecutar\n",
    "    end_time = time.time()\n",
    "    memoria_final = proceso.memory_info().rss\n",
    "    memoria_utilizada = memoria_final - memoria_inicial\n",
    "    \n",
    "    # Calcular tiempo de ejecución\n",
    "    tiempo_ejecucion = end_time - start_time\n",
    "    \n",
    "    return tiempo_ejecucion, memoria_utilizada, result\n",
    "\n",
    "# Evaluar BoW (Bag of Words)\n",
    "def evaluar_bow():\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(documentos_texto)\n",
    "    return X_bow\n",
    "\n",
    "# Evaluar TF-IDF\n",
    "def evaluar_tfidf():\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(documentos_texto)\n",
    "    return X_tfidf\n",
    "\n",
    "# Evaluar Word2Vec\n",
    "def evaluar_word2vec():\n",
    "    model_w2v = Word2Vec(sentences=[doc[\"tokens\"] for doc in documentos.values()], vector_size=100, window=5, min_count=1, workers=4)\n",
    "    def obtener_vector_promedio(tokens, model):\n",
    "        vectores = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectores:\n",
    "            return np.mean(vectores, axis=0)\n",
    "        else:\n",
    "            return np.zeros(model.vector_size)\n",
    "    documentos_w2v = {doc_id: obtener_vector_promedio(doc['tokens'], model_w2v) for doc_id, doc in documentos.items()}\n",
    "    return documentos_w2v\n",
    "\n",
    "# Evaluar cada técnica\n",
    "tiempo_bow, memoria_bow, _ = medir_tiempo_memoria(evaluar_bow)\n",
    "tiempo_tfidf, memoria_tfidf, _ = medir_tiempo_memoria(evaluar_tfidf)\n",
    "tiempo_w2v, memoria_w2v, _ = medir_tiempo_memoria(evaluar_word2vec)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Evaluación BoW: Tiempo = {tiempo_bow:.4f} segundos, Memoria = {memoria_bow / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Evaluación TF-IDF: Tiempo = {tiempo_tfidf:.4f} segundos, Memoria = {memoria_tfidf / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Evaluación Word2Vec: Tiempo = {tiempo_w2v:.4f} segundos, Memoria = {memoria_w2v / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Limpiar memoria\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión de la Evaluación\n",
    "\n",
    "- **BoW** es la técnica más rápida en términos de tiempo de ejecución y requiere una cantidad moderada de memoria para procesar los documentos. Por lo tanto, es la opción más eficiente en tiempo y memoria, ideal para aplicaciones que necesitan rapidez.\n",
    "- **TF-IDF**, aunque es un poco más lento que BoW y consume algo más de memoria, ofrece un buen equilibrio entre rendimiento y precisión. Esto se debe a que calcula pesos adicionales para las palabras, lo que incrementa el tiempo de procesamiento y la memoria.\n",
    "- **Word2Vec**, a pesar de usar menos memoria, tiene un tiempo de ejecución significativamente mayor. Esto se debe a que entrena un modelo de vectores de palabras, lo que es más costoso computacionalmente. Es más lento, pero resulta más robusto y preciso, especialmente cuando se necesita capturar relaciones semánticas entre las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.  Indexación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.  Construir un Índice invertido que mapee términos a documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el índice invertido\n",
    "def construir_indice_invertido(documentos):\n",
    "    indice_invertido = defaultdict(set)\n",
    "    \n",
    "    # Recorremos cada documento\n",
    "    for doc_id, doc in documentos.items():\n",
    "        # Recorrer cada token del documento\n",
    "        for token in doc[\"tokens\"]:\n",
    "            indice_invertido[token].add(doc_id)\n",
    "    \n",
    "    return indice_invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Término: 'bahia' -> Documentos: {'training/11459', 'test/17568', 'training/1', 'test/15580', 'test/16071', 'training/11911', 'training/13462'}\n"
     ]
    }
   ],
   "source": [
    "# Construir el índice invertido\n",
    "indice_invertido = construir_indice_invertido(documentos)\n",
    "\n",
    "# Mostrar el índice invertido para las primeras palabras\n",
    "for i, (term, docs) in enumerate(indice_invertido.items()):\n",
    "    if i >= 1:  # Limitar a las primeras 1 entradas para no saturar la salida\n",
    "        break\n",
    "    print(f\"Término: '{term}' -> Documentos: {docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "\n",
    "Con el **Índice Invertido** cada línea de la salida muestra un término (como `'bahia'` o `'cocoa'`) y los documentos (con identificadores como `'training/11459'`) en los que dicho término aparece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2.  Implementar y optimizar estructuras de datos para el Índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo para construir el índice invertido: 0.3150 segundos\n",
      "Memoria utilizada: 27.08 MB\n",
      "Término: 'bahia' -> Documentos: {'training/11459', 'test/17568', 'training/1', 'test/15580', 'test/16071', 'training/11911', 'training/13462'}\n"
     ]
    }
   ],
   "source": [
    "# Función para medir el tiempo y la memoria de la construcción del índice\n",
    "def medir_tiempo_memoria_indice(func, documentos):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Medir la memoria antes de ejecutar\n",
    "    proceso = psutil.Process()\n",
    "    memoria_inicial = proceso.memory_info().rss\n",
    "    \n",
    "    # Ejecutar la función para construir el índice\n",
    "    indice = func(documentos)\n",
    "    \n",
    "    # Medir el tiempo y la memoria después de ejecutar\n",
    "    end_time = time.time()\n",
    "    memoria_final = proceso.memory_info().rss\n",
    "    memoria_utilizada = memoria_final - memoria_inicial\n",
    "    \n",
    "    tiempo_ejecucion = end_time - start_time\n",
    "    \n",
    "    return tiempo_ejecucion, memoria_utilizada, indice\n",
    "\n",
    "# Evaluar el índice invertido\n",
    "tiempo_indice, memoria_indice, indice = medir_tiempo_memoria_indice(construir_indice_invertido, documentos)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Tiempo para construir el índice invertido: {tiempo_indice:.4f} segundos\")\n",
    "print(f\"Memoria utilizada: {memoria_indice / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Mostrar algunos términos en el índice invertido para verificar\n",
    "for termino, docs in list(indice.items())[:1]:  # Muestra los primeros 5 términos\n",
    "    print(f\"Término: '{termino}' -> Documentos: {docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Diseño del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1.  Desarrollar la lógica para procesar consultas de usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar una consulta utilizando las funciones de limpieza y tokenización\n",
    "def procesar_consulta(consulta, indice_invertido):\n",
    "    # Limpieza de la consulta\n",
    "    consulta_limpia = limpiar_texto(consulta)\n",
    "    \n",
    "    # Tokenización de la consulta\n",
    "    consulta_tokens = tokenizar_texto(consulta_limpia)\n",
    "    \n",
    "    # Inicializar un conjunto para los documentos relevantes\n",
    "    documentos_relevantes = set()\n",
    "    \n",
    "    # Buscar cada término en el índice invertido\n",
    "    for termino in consulta_tokens:\n",
    "        if termino in indice_invertido:\n",
    "            documentos_relevantes.update(indice_invertido[termino])\n",
    "    \n",
    "    return documentos_relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos que contienen los términos de la consulta: {'training/11459', 'training/1', 'test/15580', 'test/17568', 'test/16071', 'training/11911', 'training/13462'}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "consulta_usuario = \"bahia\"  # Ejemplo de consulta del usuario\n",
    "documentos_encontrados = procesar_consulta(consulta_usuario, indice_invertido)\n",
    "\n",
    "print(f\"Documentos que contienen los términos de la consulta: {documentos_encontrados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2. Utilizar algoritmos de similitud como similitud coseno o Jaccard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular similitud de coseno\n",
    "def similitud_coseno(consulta, documentos):\n",
    "    vectorizador = TfidfVectorizer()\n",
    "    \n",
    "    # Extraer textos de los documentos\n",
    "    textos_documentos = [documentos[doc_id]['texto'] for doc_id in documentos]\n",
    "    \n",
    "    # Crear el corpus con la consulta y los textos relevantes\n",
    "    corpus = [consulta] + textos_documentos\n",
    "    \n",
    "    # Generar la matriz TF-IDF\n",
    "    matriz_tfidf = vectorizador.fit_transform(corpus)\n",
    "    \n",
    "    # Calcular la similitud de coseno entre la consulta y cada documento\n",
    "    similitudes = cosine_similarity(matriz_tfidf[0:1], matriz_tfidf[1:]).flatten()\n",
    "    \n",
    "    # Retornar resultados como un diccionario\n",
    "    return dict(zip(documentos.keys(), similitudes))\n",
    "\n",
    "# Función para calcular similitud de Jaccard\n",
    "def similitud_jaccard(consulta, documentos):\n",
    "    tokens_consulta = set(tokenizar_texto(limpiar_texto(consulta)))\n",
    "    similitudes = {}\n",
    "    \n",
    "    for doc_id, doc in documentos.items():\n",
    "        tokens_doc = set(doc['tokens'])\n",
    "        interseccion = len(tokens_consulta & tokens_doc)\n",
    "        union = len(tokens_consulta | tokens_doc)\n",
    "        similitudes[doc_id] = interseccion / union if union > 0 else 0\n",
    "    \n",
    "    return similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Coseno:\n",
      "training/11459: 0.0368\n",
      "training/1: 0.0552\n",
      "test/15580: 0.0687\n",
      "test/17568: 0.0346\n",
      "test/16071: 0.0706\n",
      "training/11911: 0.1087\n",
      "training/13462: 0.0067\n",
      "\n",
      "Similitud de Jaccard:\n",
      "training/11459: 0.0100\n",
      "training/1: 0.0079\n",
      "test/15580: 0.0370\n",
      "test/17568: 0.0204\n",
      "test/16071: 0.0233\n",
      "training/11911: 0.0345\n",
      "training/13462: 0.0058\n"
     ]
    }
   ],
   "source": [
    "# Proceso principal\n",
    "consulta_usuario = \"bahia\"  # Consulta de ejemplo\n",
    "documentos_encontrados_ids = procesar_consulta(consulta_usuario, indice_invertido)\n",
    "\n",
    "# Filtrar los documentos encontrados\n",
    "documentos_encontrados = {doc_id: documentos[doc_id] for doc_id in documentos_encontrados_ids}\n",
    "\n",
    "if documentos_encontrados:\n",
    "    print(\"Similitud de Coseno:\")\n",
    "    resultados_coseno = similitud_coseno(consulta_usuario, documentos_encontrados)\n",
    "    for doc, sim in resultados_coseno.items():\n",
    "        print(f\"{doc}: {sim:.4f}\")\n",
    "    \n",
    "    print(\"\\nSimilitud de Jaccard:\")\n",
    "    resultados_jaccard = similitud_jaccard(consulta_usuario, documentos_encontrados)\n",
    "    for doc, sim in resultados_jaccard.items():\n",
    "        print(f\"{doc}: {sim:.4f}\")\n",
    "else:\n",
    "    print(\"No se encontraron documentos relevantes para la consulta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2. Desarrollar un algoritmo de ranking para ordenar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankear_documentos(resultados_coseno, resultados_jaccard):\n",
    "    # Rankear documentos por similitud de coseno\n",
    "    documentos_rankeados_coseno = sorted(resultados_coseno.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Rankear documentos por similitud de Jaccard\n",
    "    documentos_rankeados_jaccard = sorted(resultados_jaccard.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return documentos_rankeados_coseno, documentos_rankeados_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos ordenados por similitud de Coseno:\n",
      "training/11911: 0.1087\n",
      "test/16071: 0.0706\n",
      "test/15580: 0.0687\n",
      "training/1: 0.0552\n",
      "training/11459: 0.0368\n",
      "test/17568: 0.0346\n",
      "training/13462: 0.0067\n",
      "\n",
      "Documentos ordenados por similitud de Jaccard:\n",
      "test/15580: 0.0370\n",
      "training/11911: 0.0345\n",
      "test/16071: 0.0233\n",
      "test/17568: 0.0204\n",
      "training/11459: 0.0100\n",
      "training/1: 0.0079\n",
      "training/13462: 0.0058\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el algoritmo de ranking\n",
    "documentos_rankeados_coseno, documentos_rankeados_jaccard = rankear_documentos(resultados_coseno, resultados_jaccard)\n",
    "\n",
    "# Mostrar documentos ordenados por similitud de Coseno\n",
    "print(\"Documentos ordenados por similitud de Coseno:\")\n",
    "for doc, puntaje in documentos_rankeados_coseno:\n",
    "    print(f\"{doc}: {puntaje:.4f}\")\n",
    "\n",
    "# Mostrar documentos ordenados por similitud de Jaccard\n",
    "print(\"\\nDocumentos ordenados por similitud de Jaccard:\")\n",
    "for doc, puntaje in documentos_rankeados_jaccard:\n",
    "    print(f\"{doc}: {puntaje:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Razonamienta pesos de similitud coseno y jaccard\n",
    "\n",
    "- **Coseno (0.7):** Se le da más peso porque mide relaciones contextuales y distribucionales, especialmente útil para documentos largos.\n",
    "- **Jaccard (0.3):** Es útil para medir la presencia exacta de términos, pero en general es más limitado en su aplicabilidad comparado con la similitud de Coseno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Diseño del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1.  Definir un conjunto de métricas de evaluación (precisión, recall, F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.  Realizar pruebas utilizando el conjunto de prueba del corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3.   Comparar el rendimiento de diferentes configuraciones del sistema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
