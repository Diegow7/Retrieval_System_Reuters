{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Bimestral: Sistema de Recuperaci´on de Información basado en Reuters-21578\n",
    "## 1. Introducción\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578.\n",
    "## 2. Fases del Proyecto\n",
    "###  2.1. Adquisición de datos\n",
    "\n",
    "*   Descargar el Corpus Reuters-21578\n",
    "*   Descomprimir y organizar los archivos\n",
    "*   Documentar el proceso de adquisición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descripción de la imagen](images/corpus_reuters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarga y análisis inicial del corpus\n",
    "\n",
    "1. El archivo comprimido fue descargado y descomprimido en una carpeta local vinculada a un repositorio de GitHub.\n",
    "2. Posteriormente, se analizó el contenido del corpus, obteniendo los siguientes resultados:\n",
    "   - **Carpeta `test`**: Contiene 3019 archivos.\n",
    "   - **Carpeta `training`**: Contiene 7769 archivos.\n",
    "   - **Archivo `cats`**: Incluye las categorías.\n",
    "   - **Archivo `readme`**: Proporciona información general sobre el corpus.\n",
    "   - **Archivo `stopwords`**: Contiene una lista de palabras vacías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerias y Dependencias necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install rarfile` para tener la biblioteca `rarfile` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `!pip install nltk` para tener la biblioteca `nltk` en nuestro entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comando `pip install scikit-learn` instala la librería `scikit-learn`, que es una herramienta para realizar tareas de aprendizaje automático (machine learning) como clasificación, regresión y clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comando `pip install gensim` instala la librería `gensim`, que se utiliza para el procesamiento de texto y la creación de modelos de aprendizaje automático, como Word2Vec, para generar representaciones vectoriales de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import rarfile\n",
    "import time # Evaluar tecnicas de vectorización\n",
    "import psutil # Evaluar tecnicas de vectorización\n",
    "import gc # Evaluar tecnicas de vectorización\n",
    "import numpy as np\n",
    "from collections import defaultdict # Para el indice invertido\n",
    "from sklearn.preprocessing import normalize # para Word2Vec\n",
    "from nltk.tokenize import word_tokenize # para tokenizar\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Extraer el contenido relevante de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos extraídos:\n",
      "['reuters']\n"
     ]
    }
   ],
   "source": [
    "rarfile.UNRAR_TOOL = r\"D:\\UnRAR\\UnRAR.exe\" # Cambiar por ubicación local de tu herramienta UNRAR\n",
    "\n",
    "# Ruta del archivo .rar\n",
    "rar_path = 'material/reuters.rar'\n",
    "output_dir = 'material/content'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Descomprimir el archivo .rar\n",
    "with rarfile.RarFile(rar_path) as rf:\n",
    "    rf.extractall(output_dir)\n",
    "\n",
    "# Verificar que se haya descomprimido correctamente\n",
    "print(\"Archivos extraídos:\")\n",
    "print(os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de documentos\n",
    "train_dir = 'material/content/reuters/training'\n",
    "test_dir = 'material/content/reuters/test'\n",
    "cats_file = 'material/content/reuters/cats.txt'\n",
    "\n",
    "# Diccionario para almacenar documentos\n",
    "documentos = {}\n",
    "\n",
    "# Función para extraer contenido de un archivo de noticias\n",
    "def extraer_texto(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            contenido = file.read()\n",
    "            texto_limpio = contenido.strip()  # Elimina espacios en blanco iniciales y finales\n",
    "            return texto_limpio\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Función para cargar las categorías de los documentos\n",
    "def cargar_categorias(filepath):\n",
    "    categorias = {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as file:\n",
    "            for linea in file:\n",
    "                partes = linea.strip().split()  # Divide la línea en partes separadas por espacios\n",
    "                if len(partes) >= 2:\n",
    "                    doc_id = partes[0]  # Primer elemento es el id del documento (test/14826, training/1)\n",
    "                    etiquetas = partes[1:]  # Resto de elementos son categorías\n",
    "                    categorias[doc_id] = etiquetas\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo de categorías {filepath}: {e}\")\n",
    "    return categorias\n",
    "\n",
    "# Función para cargar los documentos y asociar categorías\n",
    "def cargar_documentos(directorio, tipo, categorias_dict):\n",
    "    archivos = os.listdir(directorio)\n",
    "    if not archivos:\n",
    "        print(f\"No se encontraron archivos en {directorio}\")\n",
    "    for archivo in archivos:\n",
    "        filepath = os.path.join(directorio, archivo)\n",
    "        doc_id = f\"{tipo}/{archivo}\"\n",
    "        texto = extraer_texto(filepath)\n",
    "        categorias = categorias_dict.get(doc_id, [])  # Obtener categorías, si no hay, devuelve lista vacía\n",
    "\n",
    "        # Almacenar en el diccionario\n",
    "        documentos[doc_id] = {\n",
    "            \"texto\": texto,\n",
    "            \"categorias\": categorias\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos cargados: 10788\n",
      "ID: training/1\n",
      "Texto: BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating the drought since early\n",
      "  January and improving prospects for the coming temporao,\n",
      "  although normal humidity levels have not been restored,\n",
      "  Comissaria Smith said in its weekly review.\n",
      "      The dry period means the temporao will be late this year.\n",
      "      Arrivals for the week ended February 22 were 155,221 bags\n",
      "  of 60 kilos making a cumulative total for the season of 5.93\n",
      "  mln against 5.81 at th...\n",
      "Categorías: ['cocoa']\n"
     ]
    }
   ],
   "source": [
    "# Cargar categorías\n",
    "categorias_dict = cargar_categorias(cats_file)\n",
    "\n",
    "# Cargar documentos de entrenamiento y prueba\n",
    "cargar_documentos(train_dir, 'training', categorias_dict)\n",
    "cargar_documentos(test_dir, 'test', categorias_dict)\n",
    "\n",
    "# Verificar el tamaño y algunas muestras\n",
    "print(f\"Total de documentos cargados: {len(documentos)}\")\n",
    "if documentos:\n",
    "    ejemplo_doc = list(documentos.items())[0]\n",
    "    print(f\"ID: {ejemplo_doc[0]}\")\n",
    "    print(f\"Texto: {ejemplo_doc[1]['texto'][:500]}...\")  # Mostrar los primeros 500 caracteres\n",
    "    print(f\"Categorías: {ejemplo_doc[1]['categorias']}\")\n",
    "else:\n",
    "    print(\"No se cargaron documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Realizar limpieza de datos: eliminación de caracteres no deseados, normalización de texto, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    # 1. Conversión a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # 2. Eliminación de caracteres especiales y números\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "\n",
    "    # 3. Eliminación de espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Texto limpio: bahia cocoa review showers continued throughout the week in the bahia cocoa zone alleviating the drought since early january and improving prospects for the coming temporao although normal humidity levels have not been restored comissaria smith said in its weekly review the dry period means the temporao will be late this year arrivals for the week ended february were bags of kilos making a cumulative total for the season of mln against at the same stage last year again it seems that cocoa delive...\n"
     ]
    }
   ],
   "source": [
    "# Aplicar limpieza de texto a todos los documentos\n",
    "for doc_id in documentos:\n",
    "    texto_original = documentos[doc_id]['texto']\n",
    "    texto_limpio = limpiar_texto(texto_original)\n",
    "\n",
    "    # Actualizar el texto limpio en el diccionario\n",
    "    documentos[doc_id]['texto'] = texto_limpio\n",
    "\n",
    "# Verificar el resultado de la limpieza en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Texto limpio: {ejemplo_doc[1]['texto'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Tokenización: dividir el texto en palabras o tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Al no encontrarnos en un entorno de Google Colab, sino en VS, nos basta con ejecutar una sola vez el comando `nltk.download('punkt')` o `nltk.download('punkt_tab')`, por lo cual no es necesario volver a ejcutarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar los recursos necesarios de NLTK para tokenizar texto en oraciones y palabras\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para tokenizar texto\n",
    "def tokenizar_texto(texto):\n",
    "    # Utilizamos word_tokenize de NLTK para dividir en tokens\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens: ['bahia', 'cocoa', 'review', 'showers', 'continued', 'throughout', 'the', 'week', 'in', 'the', 'bahia', 'cocoa', 'zone', 'alleviating', 'the', 'drought', 'since', 'early', 'january', 'and']...\n"
     ]
    }
   ],
   "source": [
    "# Aplicar tokenización a todos los documentos\n",
    "for doc_id in documentos:\n",
    "    texto_limpio = documentos[doc_id]['texto']\n",
    "    tokens = tokenizar_texto(texto_limpio)\n",
    "\n",
    "    # Guardamos los tokens en el diccionario\n",
    "    documentos[doc_id]['tokens'] = tokens\n",
    "\n",
    "# Verificar el resultado de la tokenización en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Eliminar stop words y  aplicar stemming o lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de Stop Words cargadas: [\"you've\", 'is', 'awfully', 'the', 'after', 'mean', 'less', \"aren't\", 'beside', 'thats', 'cause', 's', 'be', 'necessary', 'available']\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo de stopwords proporcionado\n",
    "ruta_stopwords = 'material/content/reuters/stopwords'\n",
    "stopwords_personalizadas = set()\n",
    "\n",
    "with open(ruta_stopwords, 'r') as archivo:\n",
    "    for linea in archivo:\n",
    "        palabra = linea.strip()  # Removemos espacios y saltos de línea\n",
    "        if palabra:  # Evitamos añadir líneas vacías\n",
    "            stopwords_personalizadas.add(palabra.lower())\n",
    "\n",
    "# Verificamos algunas stop words cargadas\n",
    "print(\"Ejemplo de Stop Words cargadas:\", list(stopwords_personalizadas)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens sin Stop Words: ['bahia', 'cocoa', 'review', 'showers', 'continued', 'week', 'bahia', 'cocoa', 'zone', 'alleviating', 'drought', 'early', 'january', 'improving', 'prospects', 'coming', 'temporao', 'normal', 'humidity', 'levels']...\n"
     ]
    }
   ],
   "source": [
    "# Función para eliminar stop words de los tokens\n",
    "def eliminar_stopwords(tokens, stopwords):\n",
    "    # Filtramos los tokens que no están en la lista de stopwords\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords]\n",
    "    return tokens_filtrados\n",
    "\n",
    "# Aplicamos la eliminación de stop words a cada documento\n",
    "for doc_id in documentos:\n",
    "    tokens = documentos[doc_id]['tokens']\n",
    "    tokens_sin_stopwords = eliminar_stopwords(tokens, stopwords_personalizadas)\n",
    "\n",
    "    # Guardamos los tokens filtrados\n",
    "    documentos[doc_id]['tokens'] = tokens_sin_stopwords\n",
    "\n",
    "# Verificar el resultado después de eliminar stop words en un documento de ejemplo\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens sin Stop Words: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "Para nuestro caso de estudio y aplicación de este proyecto, aplicaremos **Steming**. \n",
    "\n",
    "Como equipo, consideramos más eficiente en términos de recursos porque el stemming utiliza reglas más simples y rápidas para reducir las palabras a su raíz, mientras que la lemmatización requiere un procesamiento más complejo y el uso de diccionarios para obtener la forma base correcta de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Función para aplicar stemming a los tokens de un documento\n",
    "def aplicar_stemming(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: training/1\n",
      "Tokens con Stemming: ['bahia', 'cocoa', 'review', 'shower', 'continu', 'week', 'bahia', 'cocoa', 'zone', 'allevi', 'drought', 'earli', 'januari', 'improv', 'prospect', 'come', 'temporao', 'normal', 'humid', 'level']...\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos el stemming a los tokens de cada documento\n",
    "for doc_id in documentos:\n",
    "    tokens = documentos[doc_id]['tokens']  # Obtiene los tokens del documento\n",
    "    tokens_stemmed = aplicar_stemming(tokens)  # Aplica el stemming\n",
    "    documentos[doc_id]['tokens'] = tokens_stemmed  # Guarda los tokens procesados\n",
    "\n",
    "# Verificamos un documento de ejemplo después del stemming\n",
    "ejemplo_doc = list(documentos.items())[0]\n",
    "print(f\"ID: {ejemplo_doc[0]}\")\n",
    "print(f\"Tokens con Stemming: {ejemplo_doc[1]['tokens'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Representación de Datos en Espacio Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Utilizar técnicas como Bag of Words (BoW), TF-IDF, y Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características (Palabras) BoW: ['aa' 'aaa' 'aabex' 'aachen' 'aaminu' 'aancor' 'aap' 'aaplu' 'aar'\n",
      " 'aarnoud']...\n",
      "Forma de la matriz BoW: (10788, 26004)\n"
     ]
    }
   ],
   "source": [
    "# Crear un objeto de CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Obtener las palabras de los documentos (usando los textos limpios después de eliminar stopwords y stemming)\n",
    "documentos_texto = [\" \".join(doc[\"tokens\"]) for doc in documentos.values()]\n",
    "\n",
    "# Ajustar el modelo y transformar los documentos\n",
    "X_bow = vectorizer.fit_transform(documentos_texto)\n",
    "\n",
    "# Mostrar las características (palabras)\n",
    "print(f\"Características (Palabras) BoW: {vectorizer.get_feature_names_out()[:10]}...\")  # Mostrar las primeras 10 palabras\n",
    "\n",
    "# Mostrar la matriz de términos de frecuencia\n",
    "print(f\"Forma de la matriz BoW: {X_bow.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características (Palabras) TF-IDF: ['aa' 'aaa' 'aabex' 'aachen' 'aaminu' 'aancor' 'aap' 'aaplu' 'aar'\n",
      " 'aarnoud']...\n",
      "Forma de la matriz TF-IDF: (10788, 26004)\n"
     ]
    }
   ],
   "source": [
    "# Crear el objeto TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar y transformar los documentos\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documentos_texto)\n",
    "\n",
    "# Mostrar las características (palabras)\n",
    "print(f\"Características (Palabras) TF-IDF: {tfidf_vectorizer.get_feature_names_out()[:10]}...\")  # Primeras 10 palabras\n",
    "\n",
    "# Mostrar la matriz de TF-IDF\n",
    "print(f\"Forma de la matriz TF-IDF: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del vector Word2Vec: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo Word2Vec\n",
    "model_w2v = Word2Vec(sentences=[doc[\"tokens\"] for doc in documentos.values()], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Función para obtener el vector medio de un documento\n",
    "def obtener_vector_promedio(tokens, model):\n",
    "    # Extraer los vectores de las palabras, promediarlos y retornar el vector\n",
    "    vectores = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if vectores:\n",
    "        return np.mean(vectores, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Devuelve un vector cero si no hay palabras del vocabulario en el modelo\n",
    "\n",
    "# Obtener los vectores para cada documento\n",
    "documentos_w2v = {}\n",
    "for doc_id, doc in documentos.items():\n",
    "    vector_promedio = obtener_vector_promedio(doc['tokens'], model_w2v)\n",
    "    documentos_w2v[doc_id] = vector_promedio\n",
    "\n",
    "# Verificar el tamaño de los vectores\n",
    "print(f\"Dimensiones del vector Word2Vec: {documentos_w2v[next(iter(documentos_w2v))].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Evaluar las diferentes técnicas de vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación BoW: Tiempo = 1.0010 segundos, Memoria = 6.29 MB\n",
      "Evaluación TF-IDF: Tiempo = 0.9970 segundos, Memoria = 5.60 MB\n",
      "Evaluación Word2Vec: Tiempo = 6.1797 segundos, Memoria = 4.19 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para medir el tiempo y la memoria\n",
    "def medir_tiempo_memoria(func):\n",
    "    # Medir el tiempo de ejecución\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Medir la memoria antes de ejecutar\n",
    "    proceso = psutil.Process()\n",
    "    memoria_inicial = proceso.memory_info().rss  # Memoria en bytes\n",
    "    \n",
    "    # Ejecutar la función\n",
    "    result = func()\n",
    "    \n",
    "    # Medir el tiempo y la memoria después de ejecutar\n",
    "    end_time = time.time()\n",
    "    memoria_final = proceso.memory_info().rss\n",
    "    memoria_utilizada = memoria_final - memoria_inicial\n",
    "    \n",
    "    # Calcular tiempo de ejecución\n",
    "    tiempo_ejecucion = end_time - start_time\n",
    "    \n",
    "    return tiempo_ejecucion, memoria_utilizada, result\n",
    "\n",
    "# Evaluar BoW (Bag of Words)\n",
    "def evaluar_bow():\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(documentos_texto)\n",
    "    return X_bow\n",
    "\n",
    "# Evaluar TF-IDF\n",
    "def evaluar_tfidf():\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(documentos_texto)\n",
    "    return X_tfidf\n",
    "\n",
    "# Evaluar Word2Vec\n",
    "def evaluar_word2vec():\n",
    "    model_w2v = Word2Vec(sentences=[doc[\"tokens\"] for doc in documentos.values()], vector_size=100, window=5, min_count=1, workers=4)\n",
    "    def obtener_vector_promedio(tokens, model):\n",
    "        vectores = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectores:\n",
    "            return np.mean(vectores, axis=0)\n",
    "        else:\n",
    "            return np.zeros(model.vector_size)\n",
    "    documentos_w2v = {doc_id: obtener_vector_promedio(doc['tokens'], model_w2v) for doc_id, doc in documentos.items()}\n",
    "    return documentos_w2v\n",
    "\n",
    "# Evaluar cada técnica\n",
    "tiempo_bow, memoria_bow, _ = medir_tiempo_memoria(evaluar_bow)\n",
    "tiempo_tfidf, memoria_tfidf, _ = medir_tiempo_memoria(evaluar_tfidf)\n",
    "tiempo_w2v, memoria_w2v, _ = medir_tiempo_memoria(evaluar_word2vec)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Evaluación BoW: Tiempo = {tiempo_bow:.4f} segundos, Memoria = {memoria_bow / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Evaluación TF-IDF: Tiempo = {tiempo_tfidf:.4f} segundos, Memoria = {memoria_tfidf / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Evaluación Word2Vec: Tiempo = {tiempo_w2v:.4f} segundos, Memoria = {memoria_w2v / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Limpiar memoria\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión de la Evaluación\n",
    "\n",
    "- **BoW** es la técnica más rápida en términos de tiempo de ejecución y requiere una cantidad moderada de memoria para procesar los documentos. Por lo tanto, es la opción más eficiente en tiempo y memoria, ideal para aplicaciones que necesitan rapidez.\n",
    "- **TF-IDF**, aunque es un poco más lento que BoW y consume algo más de memoria, ofrece un buen equilibrio entre rendimiento y precisión. Esto se debe a que calcula pesos adicionales para las palabras, lo que incrementa el tiempo de procesamiento y la memoria.\n",
    "- **Word2Vec**, a pesar de usar menos memoria, tiene un tiempo de ejecución significativamente mayor. Esto se debe a que entrena un modelo de vectores de palabras, lo que es más costoso computacionalmente. Es más lento, pero resulta más robusto y preciso, especialmente cuando se necesita capturar relaciones semánticas entre las palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.  Indexación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.  Construir un Índice invertido que mapee términos a documentos.\n",
    "#### 2.4.2.  Implementar y optimizar estructuras de datos para el Índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el índice invertido extendido con TF-IDF o Word2Vec\n",
    "def construir_indice_invertido_vectorizado(documentos, vectorizador, X_vectores, metodo=\"tfidf\"):\n",
    "    indice_invertido = defaultdict(list)  # Cambiamos a lista para guardar (doc_id, peso)\n",
    "    vocabulario = vectorizador.get_feature_names_out()  # Palabras del vocabulario\n",
    "    terminos_vectores = X_vectores.toarray()  # Convertimos la matriz en un arreglo denso (para TF-IDF o BoW)\n",
    "    \n",
    "    # Recorremos cada documento y su representación vectorial\n",
    "    for doc_id, vector in zip(documentos.keys(), terminos_vectores):\n",
    "        for idx, peso in enumerate(vector):  # idx es el índice del término en el vocabulario\n",
    "            if peso > 0:  # Solo términos que aparecen en el documento\n",
    "                termino = vocabulario[idx]\n",
    "                indice_invertido[termino].append((doc_id))\n",
    "    \n",
    "    return indice_invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Término: 'allevi' -> Entradas: ['training/1', 'training/10593', 'training/11120', 'training/11135', 'training/1611', 'training/2746', 'training/5717', 'training/5826', 'test/17502', 'test/17980', 'test/18009', 'test/18744', 'test/19165']\n"
     ]
    }
   ],
   "source": [
    "# Construcción del índice usando TF-IDF\n",
    "indice_invertido_tfidf = construir_indice_invertido_vectorizado(\n",
    "    documentos=documentos,\n",
    "    vectorizador=tfidf_vectorizer,\n",
    "    X_vectores=X_tfidf,\n",
    "    metodo=\"tfidf\"\n",
    ")\n",
    "\n",
    "# Mostrar los primeros términos del índice invertido extendido con TF-IDF\n",
    "for i, (termino, entradas) in enumerate(indice_invertido_tfidf.items()):\n",
    "    if i >= 1:  # Mostrar solo los primeros 1 términos\n",
    "        break\n",
    "    print(f\"Término: '{termino}' -> Entradas: {entradas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_indice_invertido_word2vec(documentos, documentos_w2v):\n",
    "    indice_invertido = defaultdict(set)  # Usamos un set para evitar duplicados\n",
    "    \n",
    "    for doc_id, doc in documentos.items():\n",
    "        vector_documento = documentos_w2v[doc_id]\n",
    "        for token in doc[\"tokens\"]:\n",
    "            indice_invertido[token].add(doc_id)  # Añadir el doc_id al set\n",
    "    \n",
    "    return indice_invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Término: 'bahia' -> Entradas: {'training/1', 'test/17568', 'test/16071', 'training/11459', 'training/13462', 'test/15580', 'training/11911'}\n"
     ]
    }
   ],
   "source": [
    "# Construcción del índice usando Word2Vec\n",
    "indice_invertido_w2v = construir_indice_invertido_word2vec(documentos, documentos_w2v)\n",
    "\n",
    "# Mostrar los primeros términos del índice invertido extendido con Word2Vec\n",
    "for i, (termino, entradas) in enumerate(indice_invertido_w2v.items()):\n",
    "    if i >= 1:  # Mostrar solo los primeros 1 términos\n",
    "        break\n",
    "    print(f\"Término: '{termino}' -> Entradas: {entradas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "\n",
    "Con el **Índice Invertido** cada línea de la salida muestra un término (como `'bahia'` o `'allevi'`) y los documentos (con identificadores como `'training/11459'`) en los que dicho término aparece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Diseño del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1.  Desarrollar la lógica para procesar consultas de usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar una consulta utilizando las funciones de limpieza y tokenización\n",
    "def procesar_consulta(consulta, indice_invertido_metodo):\n",
    "    # Limpieza de la consulta\n",
    "    consulta_limpia = limpiar_texto(consulta)\n",
    "    \n",
    "    # Tokenización de la consulta\n",
    "    consulta_tokens = tokenizar_texto(consulta_limpia)\n",
    "    \n",
    "    # Inicializar un conjunto para los documentos relevantes\n",
    "    documentos_relevantes = set()\n",
    "    \n",
    "    # Buscar cada término en el índice invertido\n",
    "    for termino in consulta_tokens:\n",
    "        if termino in indice_invertido_metodo:\n",
    "            documentos_relevantes.update(indice_invertido_metodo[termino])\n",
    "    \n",
    "    return documentos_relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2. Utilizar algoritmos de similitud como similitud coseno o Jaccard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TF-IDF:** Utiliza la frecuencia ponderada de los términos en los documentos. Cuando usas similitud de coseno con un índice basado en TF-IDF, lo que estás comparando es qué tan similar es un documento en relación con la consulta, tomando en cuenta la frecuencia y la importancia relativa de los términos en el corpus.\n",
    "- **Word2Vec (w2v):** Representa las palabras en un espacio vectorial continuo donde la similitud de coseno se usa para medir qué tan cerca están dos vectores semánticamente. Este enfoque es diferente de TF-IDF porque no considera las frecuencias de las palabras, sino que se centra en el contexto semántico y las relaciones entre palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular similitud de coseno\n",
    "def similitud_coseno_tfidf(consulta, documentos):\n",
    "    vectorizador = TfidfVectorizer()\n",
    "    \n",
    "    # Extraer textos de los documentos\n",
    "    textos_documentos = [documentos[doc_id]['texto'] for doc_id in documentos]\n",
    "    \n",
    "    # Crear el corpus con la consulta y los textos relevantes\n",
    "    corpus = [consulta] + textos_documentos\n",
    "    \n",
    "    # Generar la matriz TF-IDF\n",
    "    matriz_tfidf = vectorizador.fit_transform(corpus)\n",
    "    \n",
    "    # Calcular la similitud de coseno entre la consulta y cada documento\n",
    "    similitudes = cosine_similarity(matriz_tfidf[0:1], matriz_tfidf[1:]).flatten()\n",
    "    \n",
    "    # Retornar resultados como un diccionario\n",
    "    return dict(zip(documentos.keys(), similitudes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Coseno (TF-IDF):\n",
      "training/1: 0.0552\n",
      "test/17568: 0.0346\n",
      "test/16071: 0.0706\n",
      "training/11459: 0.0368\n",
      "training/13462: 0.0067\n",
      "test/15580: 0.0687\n",
      "training/11911: 0.1087\n"
     ]
    }
   ],
   "source": [
    "# Consulta de ejemplo\n",
    "consulta_usuario = \"bahia\"  # Consulta de ejemplo\n",
    "documentos_encontrados_ids = procesar_consulta(consulta_usuario, indice_invertido_tfidf)\n",
    "\n",
    "# Filtrar los documentos encontrados\n",
    "documentos_encontrados = {doc_id: documentos[doc_id] for doc_id in documentos_encontrados_ids}\n",
    "\n",
    "if documentos_encontrados:\n",
    "    print(\"Similitud de Coseno (TF-IDF):\")\n",
    "    resultados_coseno_tfidf = similitud_coseno_tfidf(consulta_usuario, documentos_encontrados)\n",
    "    for doc, sim in resultados_coseno_tfidf.items():\n",
    "        print(f\"{doc}: {sim:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_promedio(texto, modelo):\n",
    "    palabras = texto.split()\n",
    "    vectores = [modelo.wv[word] for word in palabras if word in modelo.wv]\n",
    "    if vectores:\n",
    "        return np.mean(vectores, axis=0)\n",
    "    else:\n",
    "        return np.zeros(modelo.vector_size)\n",
    "\n",
    "# Generar vectores para todos los documentos\n",
    "documentos_vectores = {}\n",
    "for doc_id, doc_info in documentos.items():\n",
    "    texto = doc_info['texto']\n",
    "    documentos_vectores[doc_id] = vector_promedio(texto, model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similitud_coseno_w2v(consulta, documentos_vectores, modelo):\n",
    "    # Calcular el vector promedio de la consulta\n",
    "    vector_consulta = vector_promedio(consulta, modelo)\n",
    "    \n",
    "    # Similaridad entre la consulta y los vectores precomputados de los documentos\n",
    "    similitudes = {}\n",
    "    for doc_id, vector_doc in documentos_vectores.items():\n",
    "        similitud = cosine_similarity([vector_consulta], [vector_doc])[0][0]\n",
    "        similitudes[doc_id] = similitud\n",
    "    \n",
    "    return similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Coseno (Word2Vec):\n",
      "training/1: 0.6896\n",
      "training/11459: 0.7244\n",
      "test/17568: 0.6927\n",
      "training/13462: 0.6669\n",
      "test/15580: 0.9252\n",
      "test/16071: 0.7133\n",
      "training/11911: 0.8943\n"
     ]
    }
   ],
   "source": [
    "# Consulta de ejemplo\n",
    "consulta_usuario = \"bahia\"\n",
    "\n",
    "# Procesar la consulta para obtener documentos relevantes\n",
    "documentos_encontrados_ids = procesar_consulta(consulta_usuario, indice_invertido_w2v)\n",
    "\n",
    "# Filtrar los documentos encontrados\n",
    "documentos_encontrados_w2v = {doc_id: documentos_w2v[doc_id] for doc_id in documentos_encontrados_ids}\n",
    "\n",
    "# Calcular similitud de coseno usando Word2Vec\n",
    "if documentos_encontrados_w2v:\n",
    "    print(\"Similitud de Coseno (Word2Vec):\")\n",
    "    resultados_coseno_w2v = similitud_coseno_w2v(consulta_usuario, documentos_encontrados_w2v, model_w2v)\n",
    "    for doc, sim in resultados_coseno_w2v.items():\n",
    "        print(f\"{doc}: {sim:.4f}\")\n",
    "else:\n",
    "    print(\"No se encontraron documentos relevantes para la consulta.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2. Desarrollar un algoritmo de ranking para ordenar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_tfidf(similitudes):\n",
    "    # Ordenar por valor de similitud (descendente)\n",
    "    return sorted(similitudes.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking de documentos usando TF-IDF:\n",
      "training/11911: 0.1087\n",
      "test/16071: 0.0706\n",
      "test/15580: 0.0687\n",
      "training/1: 0.0552\n",
      "training/11459: 0.0368\n",
      "test/17568: 0.0346\n",
      "training/13462: 0.0067\n"
     ]
    }
   ],
   "source": [
    "# Similitudes calculadas previamente con TF-IDF\n",
    "resultados_coseno_tfidf = similitud_coseno_tfidf(consulta_usuario, documentos_encontrados)\n",
    "\n",
    "# Aplicar el ranking\n",
    "ranking_resultados_tfidf = ranking_tfidf(resultados_coseno_tfidf)\n",
    "\n",
    "# Mostrar resultados ordenados\n",
    "print(\"Ranking de documentos usando TF-IDF:\")\n",
    "for doc, sim in ranking_resultados_tfidf:\n",
    "    print(f\"{doc}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_w2v(similitudes):\n",
    "    # Ordenar por valor de similitud (descendente)\n",
    "    return sorted(similitudes.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking de documentos usando Word2Vec:\n",
      "test/15580: 0.9252\n",
      "training/11911: 0.8943\n",
      "training/11459: 0.7244\n",
      "test/16071: 0.7133\n",
      "test/17568: 0.6927\n",
      "training/1: 0.6896\n",
      "training/13462: 0.6669\n"
     ]
    }
   ],
   "source": [
    "# Similitudes calculadas previamente con Word2Vec\n",
    "resultados_coseno_w2v = similitud_coseno_w2v(consulta_usuario, documentos_encontrados_w2v, model_w2v)\n",
    "\n",
    "# Aplicar el ranking\n",
    "ranking_resultados_w2v = ranking_w2v(resultados_coseno_w2v)\n",
    "\n",
    "# Mostrar resultados ordenados\n",
    "print(\"Ranking de documentos usando Word2Vec:\")\n",
    "for doc, sim in ranking_resultados_w2v:\n",
    "    print(f\"{doc}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencias entre TF-IDF y Word2Vec\n",
    "\n",
    "**TF-IDF:** \n",
    "- Se basa en la frecuencia de las palabras dentro de los documentos y en el corpus completo.\n",
    "- Es un enfoque puramente estadístico y no considera el significado semántico de las palabras.\n",
    "- La similitud de coseno con TF-IDF mide cuánto se parecen los vectores de frecuencia/peso de las palabras entre la consulta y los documentos.\n",
    "\n",
    "**Word2Vec:** \n",
    "- Se basa en representaciones vectoriales aprendidas (embeddings) que capturan relaciones semánticas entre palabras.\n",
    "- Si palabras similares (como sinónimos o contextualmente relacionadas) aparecen en los documentos y en la consulta, Word2Vec detectará esta relación incluso si no son exactamente las mismas palabras.\n",
    "- Los vectores promedios de Word2Vec reflejan la semántica del texto más que la frecuencia.\n",
    "\n",
    "#### ¿Por qué el orden cambia?\n",
    "\n",
    "**TF-IDF:** \n",
    "- *En TF-IDF*, un documento con una palabra clave más frecuente (o con menos documentos relevantes en el corpus) tendrá una mayor similitud.\n",
    "- *En Word2Vec*, la similitud puede ser más alta para documentos que contienen palabras contextualmente relacionadas o con una mayor densidad de conceptos semánticamente relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Diseño del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1.  Definir un conjunto de métricas de evaluación (precisión, recall, F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nota\n",
    "\n",
    "**TF-IDF:** \n",
    "- *Resultados*: Lista o conjunto de documentos recuperados por el sistema.\n",
    "- *Relevantes*: Lista o conjunto de documentos que son relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular métricas de evaluación\n",
    "def calcular_metricas(resultados, relevantes):\n",
    "    # Convertir resultados y relevantes a conjuntos (para evitar duplicados)\n",
    "    resultados = set(resultados)\n",
    "    relevantes = set(relevantes)\n",
    "    \n",
    "    # Calcular intersección de documentos recuperados y relevantes\n",
    "    relevantes_recuperados = resultados.intersection(relevantes)\n",
    "    \n",
    "    # Precisión\n",
    "    precision = len(relevantes_recuperados) / len(resultados) if resultados else 0\n",
    "    \n",
    "    # Recall\n",
    "    recall = len(relevantes_recuperados) / len(relevantes) if relevantes else 0\n",
    "    \n",
    "    # F1-Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "    \n",
    "    return {\n",
    "        \"precisión\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"F1-score\": f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas para TF-IDF:\n",
      "precisión: 0.5714\n",
      "recall: 1.0000\n",
      "F1-score: 0.7273\n"
     ]
    }
   ],
   "source": [
    "# Definir el umbral para determinar documentos relevantes (TF-IDF)\n",
    "umbral = 0.05\n",
    "\n",
    "# Documentos relevantes basados en el umbral\n",
    "relevantes_tfidf = [doc for doc, sim in ranking_resultados_tfidf if sim > umbral]\n",
    "\n",
    "# Todos los documentos recuperados por el sistema\n",
    "resultados_tfidf = [doc for doc, sim in ranking_resultados_tfidf]\n",
    "\n",
    "# Calcular métricas\n",
    "metricas_tfidf = calcular_metricas(resultados_tfidf, relevantes_tfidf)\n",
    "\n",
    "# Mostrar métricas\n",
    "print(\"Métricas para TF-IDF:\")\n",
    "for metrica, valor in metricas_tfidf.items():\n",
    "    print(f\"{metrica}: {valor:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas para Word2Vec:\n",
      "precisión: 0.5714\n",
      "recall: 1.0000\n",
      "F1-score: 0.7273\n"
     ]
    }
   ],
   "source": [
    "# Definir el umbral para determinar documentos relevantes (Word2Vec)\n",
    "umbral_w2v = 0.7\n",
    "\n",
    "# Documentos relevantes basados en el umbral\n",
    "relevantes_w2v = [doc for doc, sim in ranking_resultados_w2v if sim > umbral_w2v]\n",
    "\n",
    "# Todos los documentos recuperados por el sistema\n",
    "resultados_w2v = [doc for doc, sim in ranking_resultados_w2v]\n",
    "\n",
    "# Calcular métricas\n",
    "metricas_w2v = calcular_metricas(resultados_w2v, relevantes_w2v)\n",
    "\n",
    "# Mostrar métricas para Word2Vec\n",
    "print(\"Métricas para Word2Vec:\")\n",
    "for metrica, valor in metricas_w2v.items():\n",
    "    print(f\"{metrica}: {valor:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.2.  Realizar pruebas utilizando el conjunto de prueba del corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3.   Comparar el rendimiento de diferentes configuraciones del sistema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
